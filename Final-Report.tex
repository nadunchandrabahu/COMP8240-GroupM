% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  11pt,
]{article}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[a4paper]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Replication of DistilBERT evaluation results},
  pdfauthor={Group M - Nadun Chandrabahu, Tai Ho, Muhammad Umer Bashir, Thanushreyas},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Replication of DistilBERT evaluation results}
\author{Group M - Nadun Chandrabahu, Tai Ho, Muhammad Umer Bashir,
Thanushreyas}
\date{November 1, 2024}

\begin{document}
\maketitle

\section{Introduction}\label{introduction}

The paper, (Sanh et al, 2019)\footnote{(Sanh et al, 2019:
  \url{https://arxiv.org/pdf/1910.01108})} ``DistilBERT, a distilled
version of BERT: smaller, faster, cheaper and lighter'', we selected for
this project presents a method for training a more compact
general-purpose language representation model called DistilBERT, which
is a streamlined version of the larger BERT model (Bidirectional Encoder
Representations from Transformers). Although BERT is highly effective
for Natural Language Processing (NLP) tasks, its size and complexity can
lead to slower inference times, particularly in on-the-edge or
resource-constrained environments, potentially impacting user
experience. This paper aims to pre-train the DistilBERT model to enhance
inference performance by reducing both its size and inference time while
maintaining accuracy. According to the paper, DistilBERT achieves a 40\%
reduction in model size, preserves 97\% of BERT's language understanding
capabilities, and improves processing speed by 60\%.

\section{Project Justification}\label{project-justification}

In this project, we will not replicate the transfer learning or model
training processes of DistilBERT. Instead, we aim to reproduce the
evaluation metrics shown in Tables 1 and 2 on page 3 of the referenced
research paper, across various datasets and downstream tasks. This will
allow us to assess DistilBERT's performance both on the datasets from
the paper and on some of our own datasets.

The DistilBERT model is available for use through Hugging Face's
transformers library (Wolf et al., 2019). In the original research,
DistilBERT was evaluated using three datasets: the GLUE (General
Language Understanding and Evaluation) benchmark \footnote{GLUE
  Benchmark: \url{https://gluebenchmark.com/}}, which includes nine
sentence-pair language understanding tasks; IMDb \footnote{IMDb dataset:
  \url{https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews}},
used for sentiment classification of user movie reviews; and SQuAD
\footnote{SQuAD v1.1 dataset:
  \url{https://rajpurkar.github.io/SQuAD-explorer/}} (Stanford Question
Answering Dataset), designed for question-answering based on a provided
passage.

We will run some Python scripts to make predictions on the data with the
DistilBERT model and calculate various metrics: the average score across
the nine GLUE benchmark tasks, test accuracy on the IMDb dataset, and
the Exact Match (EM) and F1 score on the SQuAD task. These metrics are
reported in Tables 1 and 2 of the research paper and it is the aim of
this project to replicate these results. Each group member will also
show the metrics using new data sources to perform similar NLP tasks
with DistilBERT.

\section{DistilBERT}\label{distilbert}

DistilBERT is a student model of BERT

The trained model is already available through a python library called
transformers which is provided by hugging face.com (add reference).
There are a few variations of the model available as well depending on
what kind of data the model was trained on.

online link to SQuAD data set:
\href{https://github.com/rajpurkar/SQuAD-explorer/blob/master/dataset/dev-v1.1.json}{https://github.com/rajpurkar/SQuAD-explorer/blob/master/dataset/dev-v1.1.jsonhttps://github.com/rajpurkar/SQuAD-explorer/blob/master/dataset/dev-v1.1.json}

\subsection{Evaluation Datasets}\label{evaluation-datasets}

We'll have a look at the benchmark datasets GLUE, IMdb, SQuAD, we can
also talk about how we can use

\subsection{Replication of Evaluation
Results}\label{replication-of-evaluation-results}

Show the coding and use the model on the datasets used in the paper such
as GLUE benchmark, IMdb, SQuAD. Calculate the metrics and show how close
they are to the ones in the paper

Include link to our github repo. And how we can replicate. Maybe we can
include a notebook file running the evaluation on the datasets. We're
using python version 3.12.7 to install transformers library so we can
get the model. Also need to install pytorch

We can run python code as follows, calculate metrics and report on
them.~Let's have a discussion about the results on the datasets used in
the paper in this section as well.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ transformers }\ImportTok{import}\NormalTok{ pipeline}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Hello GroupM!"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{New data construction}\label{new-data-construction}

Each member can talk about how they constructed their data.

\subsubsection{Tai Ho's dataset}\label{tai-hos-dataset}

Talk about how you created your dataset, and what your goal /context of
the dataset is.

\subsubsection{Nadun Chandrahu's
dataset}\label{nadun-chandrahus-dataset}

I'll be talking about a question and answer dataset, similar to SQuAD, I
can manually anotate whether the answers are correct or not.

\subsubsection{Evaluation using new
datasets}\label{evaluation-using-new-datasets}

\subsubsection{Tai Ho's dataset}\label{tai-hos-dataset-1}

Talk about how your dataset evaluates with the model

\subsubsection{Nadun Chandrabahu's
dataset}\label{nadun-chandrabahus-dataset}

Talk about how your dataset evaluates with the model

\subsection{Reflections}\label{reflections}

Each of us can write a paragraph about our reflections on the project.

\section{References}\label{references}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Sanh, V., Debut, L., Chaumond, J., \& Wolf, T. (2019). DistilBERT, a
  distilled version of BERT: smaller, faster, cheaper and lighter.
  \url{https://doi.org/10.48550/arXiv.1910.01108}
\item
  Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A.,
  Cistac, P., Rault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer,
  S., Platen, P. V., M, C., Jernite, Y., Plu, J., Xu, C., Scao, T. L.,
  Gugger, S., \ldots{} Drame, M. (2021). HuggingFace's Transformers:
  State-of-the-art Natural Language Processing. Webology.
  \url{https://doi.org/10.48550/arXiv.1910.03771}
\end{enumerate}

\end{document}
