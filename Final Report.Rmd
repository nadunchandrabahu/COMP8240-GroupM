---
title: "Replication of DistilBERT evaluation results"
author: "Group M - Nadun Chandrabahu, Tai Ho, Muhammad Umer Bashir, Thanushreyas Appaji"
date: "November 1, 2024"
output: 
  pdf_document:
    number_sections: true
    latex_engine: xelatex 
    keep_tex: true 
fontsize: 11pt
geometry: a4paper 
---

# Introduction

Reproducible research is vital in machine learning for several reasons. Firstly, it enables the validation of results, when research can be consistently reproduced, it enhances the credibility of the original findings. Additionally, reproducibility supports benchmarking for comparing algorithms and models, allowing the machine learning community to identify the best-performing models. It also plays an important role in identifying errors and biases, such as coding mistakes or data handling issues, improving the reliability of the research.

The paper that we selected for this project, (Sanh et al, 2019)[^1] "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter", presents a method for training a more compact general-purpose language representation model called DistilBERT, which is a streamlined version of the larger BERT model (Bidirectional Encoder Representations from Transformers). Although BERT is highly effective for Natural Language Processing (NLP) tasks, its size and complexity can lead to slower inference times, particularly in on-the-edge or resource-constrained environments, potentially impacting user experience. This paper aims to pre-train the DistilBERT model to enhance inference performance by reducing both its size and inference time while maintaining accuracy. According to the paper, DistilBERT achieves a 40% reduction in model size, preserves 97% of BERT's language understanding capabilities, and improves processing speed by 60%.

[^1]: (Sanh et al, 2019): <https://arxiv.org/pdf/1910.01108>

The source files and collaborative work done on this project is available for review on GitHub[^2].

[^2]: Project on GitHub: <https://github.com/nadunchandrabahu/COMP8240-GroupM>

# Project Justification

In this project, we will not replicate the transfer learning or model training processes of DistilBERT. Instead, we aim to reproduce the evaluation metrics shown in Tables 1 and 2 on page 3 of the referenced research paper, across various datasets and downstream tasks. This will allow us to assess DistilBERT's performance both on the datasets from the paper and on some of our own datasets. The DistilBERT model is available for evaluation through Hugging Face's transformers library (Wolf et al., 2019). In the original research, DistilBERT was evaluated using three datasets: the GLUE (General Language Understanding and Evaluation)[^3] benchmark, which includes nine sentence-pair language understanding tasks; IMDb (Internet Movie Database)[^4], used for sentiment classification of user movie reviews; and SQuAD (Stanford Question Answering Dataset)[^5], designed for question-answering based on a provided context.

[^3]: GLUE Benchmark: <https://gluebenchmark.com/>

[^4]: IMDb dataset: <https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews>

[^5]: SQuAD v1.1 dataset: <https://rajpurkar.github.io/SQuAD-explorer/>

We will run some python code on Jupyter/Colab notebooks to make predictions on the data with the DistilBERT model and calculate various metrics: the average score across the nine GLUE benchmark tasks, test accuracy on the IMDb dataset, and the Exact Match (EM) and F1 score on the SQuAD task. These metrics are reported in Tables 1 and 2 of the research paper and it is the aim of this project to replicate these results. Each group member will also show the metrics using new data sources to perform similar NLP tasks with DistilBERT.

Table 1 below shows the scoring of DistilBERT evaluation on 9 GLUE benchmark tasks as shown in the research paper.

```{r table1, echo=FALSE}
library(knitr)
library(kableExtra)

# Create the data
table_data <- data.frame(
  "Model Name" = c("", "BERT-base", "DistilBERT"),
  "Score" = c("", 79.5, 77.0),
  "CoLA" = c("", 56.3, 51.3),
  "MNLI" = c("", 86.7, 82.2),
  "MRPC" = c("", 88.6, 87.5),
  "QNLI" = c("", 91.8, 89.2),
  "QQP" = c("", 89.6, 88.5),
  "RTE" = c("", 69.3, 59.9),
  "SST-2" = c("", 92.7, 91.3),
  "STS-B" = c("", 89.0, 86.9),
  "WNLI" = c("", 53.5, 56.3)
)

# Create the table
kable(table_data, align = "c", caption = "BERT and DistilBERT results on GLUE tasks") %>%
  add_header_above(c(" " = 1, "Metrics" = 10)) %>%
  kable_styling(full_width = F)

```

Table 2 below shows the test accuracy of IMDb sentiment analysis tasks, and EM/F1 scores of the SQuAD question answering task as shown on the research paper.

```{r table2, echo=FALSE}
library(knitr)
library(kableExtra)

# Create the data
table_data <- data.frame(
  model_name = c("", "BERT-base", "DistilBERT"),
  accuracy = c("acc.", 93.46, 92.82),
  "ExactMatch" = c("EM", 81.2, 77.7),
  "FScore" = c("F1", 88.5, 85.8)
)

# Create the table with merged columns in the header
kable(table_data, align = "c",caption="IMDb and SQuAD metrics",col.names=c("Model Name","","","")) %>%
  add_header_above(c(" " = 1, "IMDb" = 1, "SQuAD" = 2)) %>%
  kable_styling(full_width = F)

```

# Original Datasets

## GLUE Benchmark

Describe about the dataset. Whole section 3 (Original Dataset should be 1/3rd of a page).

## IMDb dataset

Describe about the dataset. Whole section 3 (Original Dataset should be 1/3rd of a page).

## SQuAD dataset

The research paper makes use of SQuAD v1.1 development set available on GitHub[^6]. The dataset consists of 17968 question-answer combinations. Each question and answer is based on the provided context. There are multiple ground truth answers to the same question. We only need to extract the context, question and answer (answer_text) for the purpose of calculating Exact Match and F-Score.

[^6]: SQuAD v1.1 Dev set: <https://github.com/rajpurkar/SQuAD-explorer/tree/master/dataset>

```{python install_libraries_pandas, echo=FALSE, include=FALSE}
import subprocess
subprocess.run(["pip","install","pandas"])
subprocess.run(["pip","install","datasets"])
#!pip install datasets
#!pip install pandas
```

```{python SQuAD-dataset-description, echo=FALSE}
import pandas as pd
import json

def json_to_df(json_file):
    arrayForDF = []
    for current_subject in json_file['data']:
        subject = current_subject['title']
        for current_context in current_subject['paragraphs']:
            context = current_context['context']
            for current_question in current_context['qas']:
                question = current_question['question']
                if (len(question) > 2):
                    for answer in current_question['answers']:
                        answer_text = answer['text']
                        answer_start = answer['answer_start']

                        record = {
                                "answer_text": answer_text,
                                "answer_start": answer_start,
                                "question": question,
                                "context": context,
                                "subject": subject
                            }
                        arrayForDF.append(record)
    df = pd.DataFrame(arrayForDF)
    return df


json_file = open("dev-v1.1.json",)
data = json.load(json_file)
squad_df = json_to_df(data)
print("Columns of SQuAD devset:\n",squad_df.columns.tolist())

```

# Replication of Evaluation on Datasets

We are able to access the DistilBERT model from 'transformers' library provided by HuggingFace. It is essential to have pyTorch installed before using the model to make predictions. The amount of time taken to make predictions depends on the size of the dataset, computational power and NLP task it tries to achieve.

## GLUE Benchmark

Write about how you did the replication. Include link to notebook

## IMDb dataset

Write about how you did the replication. Include link to notebook

## SQuAD dataset

Evaluation of SQuAD (Stanford QUestion Answering Dataset) question answering task was performed by Nadun Chandrabahu and the Jupyter-notebook (SQuad-v1.1.ipynb) is available in the github repository.

Using the DistilBERT model, predicted answers were obtained based on the context and question. The inference time was 44 minutes on an AMD Ryzen 5 CPU with 16GB RAM. The model prediction returns a score, start and answer. I made a new column called Exact match that is either 1 or 0 if the predicted answer is the exact same as the answer_text column. And I calculated the average score when the model predicted an exact match. I obtained a result of 77.6%, while the research paper reported an EM of 77.7%.

The F-Score was calculated by using the following formula.

The F-score is given by $F1 = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}$.

I used the f1_score function from Python's scikit-learn library to calculate the F-Score, as well as precision and recall, which rely on the counts of True Positives, False Positives, and False Negatives. True Positives occur when the predicted values match the ground truth exactly. False Positives arise when the prediction overlaps with the ground truth but is not fully correct, while False Negatives are predictions that do not align with any part of the ground truth.

I obtained an F-Score of 75.2%, which is 10.6% lower than the 85.8% reported in the paper. This difference may stem from the inherent variability in calculating Precision and Recall, as the model's predictions can differ slightly with each run, significantly influencing Precision and Recall values, and hence the F-Score.

```{python, install_libraries, include=FALSE,eval=FALSE}
import subprocess
subprocess.run(["pip","install","transformers"])
subprocess.run(["pip","install","torch"])
#only install for first time run, use above when knitting:
#!pip install transformers
#!pip install torch
```

# Evaluation on New Datasets

## Nadun Chandrabahu

Evaluation of my new dataset McTest (Machine Comprehension Test)[^7] (Richardson et al, 2013) with question answering task was performed and the Jupyter-notebook (own-dataset.ipynb) is available in the github repository.

[^7]: McTest Dataset: <https://huggingface.co/datasets/sagnikrayc/mctest>

The dataset includes 600 records of the following columns:

```{python McTest, echo=FALSE}
from datasets import load_dataset
import pandas as pd

ds = load_dataset("sagnikrayc/mctest", "mc500")
ds=ds["test"]
columns = list(ds.features.keys())

# Insert newline after the 3rd column name
columns_str = ", ".join(columns[:3]) + ",\n" + ", ".join(columns[3:])
print("Columns of McTest devset:\n", columns_str)

```

The dataset can be easily loaded into Python using the `load_datasets` method from the HuggingFace datasets library. I had to combine the above two columns answer_options, which is a dictionary of all the possible multiple choice answers and answer, which is the correct choice out of A, B, C, or D, into a new column called answer_text which would act as the ground truth label. The model predictions once again included a score, start and answer (predicted answer) and the rest of the processing & evaluating of EM and F1 scores was carried out similar to how SQuAD task was evaluated.

I selected the McTest dataset due to it being another question-answering NLP task that is possible to be conducted using the DistilBERT model. Previously, we achieved an Exact Match (EM) score of 77.6% and an F1 score of 75.2% on the SQuAD question-answering task. I aimed to achieve similar results with this dataset. However, while I achieved an EM score of 76.1%, my F-Score was only 31.2%, this could be due to the ground truth labels in this dataset being formatted much differently to how DistilBERT makes predictions. As a solution, I could have manually curated the ground truth anwers to be similar to the predictions made by DistilBERT.

## Tai Ho

Write about your own dataset and metrics

## Thanushreyas Appaji

Write about your own dataset and metrics

# Reflections

Each of us can write a paragraph about our reflections on the project.

I, Nadun Chandrabahu, successfully replicated the evaluation results of DistilBERT on the SQuAD task, and explored the same metrics on McTest NLP question answering task. Although my EM scores were satisfactory, my F1 scores did not match up to expectations on both the SQuAD and MCTest question-answering tasks. This discrepancy may stem from an error in my method to F1 score calculation or could be due to the random errors in model predictions, which can notably impact the F1 score as True Positives, False Positives and False Negatives may be incorrectly counted.

# References

1.  Sanh, V., Debut, L., Chaumond, J., & Wolf, T. (2019). DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. <https://doi.org/10.48550/arXiv.1910.01108>

2.  Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer, S., Platen, P. V., M, C., Jernite, Y., Plu, J., Xu, C., Scao, T. L., Gugger, S., … Drame, M. (2021). HuggingFace's Transformers: State-of-the-art Natural Language Processing. Webology. <https://doi.org/10.48550/arXiv.1910.03771>

3.  Richardson, M., Burges, C. J., & Renshaw, E. (2013). MCTest: A Challenge Dataset for the Open-Domain Machine Comprehension of Text. EMNLP. <https://mattr1.github.io/mctest/MCTest_EMNLP2013.pdf>
